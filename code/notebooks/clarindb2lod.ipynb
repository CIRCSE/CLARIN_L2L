{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the RDF datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook, we use Python to transform the content of a relational DB into 3 RDF datasets, consisting of:\n",
    "1. the Lemma Bank (a series of instances of `ontolex:Form` used to lemmatize and index lexical resources): for the code, go to [this section](#the-lemma-bank);\n",
    "2. a frequency list, reporting the nr. of attestation of each of the extracted lemmas into the textual resources scanned in the CLARIN VLO: code in [this section](#corpus-frequencies);\n",
    "3. a lexical resource transformed into LLOD with the lemmas set to the URIs of our Lemma Bank (created in Step 1): code in [this section](#a-lexical-resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the dump of the DB created by our demo tool on Italian, imported on a local MariaDB from the file `CLARINDB.sql.tar.gz` (you can find it in the [data](../../data/) folder). This dump include the attestations of Italian lemmas in only 14 textual resources.\n",
    "\n",
    "In order to follow along with the code:\n",
    "- create a local DB called `CLARINDB`\n",
    "- import the sql dump (e.g. uncompress it and then run: `mysql -u <YOUR_USER> -p CLARINDB < CLARINDB.sql`)\n",
    "- create a ini configuration file called `dbconfig.ini` that looks like this:\n",
    "\n",
    "\n",
    "```ini\n",
    "[Database]\n",
    "host = localhost\n",
    "user = your_username\n",
    "password = your_password\n",
    "database = your_database_name\n",
    "```\n",
    "\n",
    "... or you can always edit the code in the [DB connection]() section to use e.g. a CSV dump!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `rdflib`\n",
    "- a DB connector (here, `mariadb` is used)\n",
    "- a DB called CLARINDB with the data dumped in `data/CLARINDB.sql.tar.gz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this code if you have a local DB running with the same schema as that generated by our L2L demo tool. You can use the CLARINDB.sql dump for Italian in this folder, if you want (see above for instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dbconfig.ini')\n",
    "\n",
    "# Get values from the 'Database' section\n",
    "db_host = config.get('Database', 'host')\n",
    "db_user = config.get('Database', 'user')\n",
    "db_password = config.get('Database', 'password')\n",
    "db_database = config.get('Database', 'database')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use MariaDB. Change the DB connector accordingly, in case you have other engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mariadb\n",
    "\n",
    "conn = mariadb.connect(\n",
    "        user=db_user,\n",
    "        password=db_password,\n",
    "        host=db_host,\n",
    "        database=db_database\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'SELECT * from it_lemmaBank;'\n",
    "cur.execute(q)\n",
    "res = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 'spiegare', 'VERB')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up `rdflib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define some widely used namespaces for classes, base URIs and properties and some shortcuts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import Namespace, URIRef, Graph, Literal, BNode\n",
    "from rdflib.namespace import DC, RDF, RDFS, DCTERMS, VOID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Namespaces\n",
    "marl = Namespace('http://www.gsi.dit.upm.es/ontologies/marl/ns#')\n",
    "\n",
    "lime = Namespace('http://www.w3.org/ns/lemon/lime#')\n",
    "ontolex = Namespace(\"http://www.w3.org/ns/lemon/ontolex#\")\n",
    "frac = Namespace('http://www.w3.org/ns/lemon/frac#')\n",
    "lexinfo = Namespace('https://www.lexinfo.net/ontology/3.0/lexinfo#')\n",
    "\n",
    "### URIRefs\n",
    "\n",
    "# properties\n",
    "written_rep = ontolex.writtenRep\n",
    "canonical_form = ontolex.canonicalForm\n",
    "has_pos = lexinfo['partOfSpeech']\n",
    "\n",
    "# classes\n",
    "lexical_entry = ontolex.LexicalEntry\n",
    "form = ontolex.Form\n",
    "\n",
    "a = RDF.type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we have to map the [UD Postags](https://universaldependencies.org/u/pos/index.html) to URIs in [Lexinfo](https://lexinfo.net/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_map = {\n",
    "    'NOUN': lexinfo['commonNoun'], \n",
    "    'ADV': lexinfo['adverb'],\n",
    "    'ADJ': lexinfo['adjective'],\n",
    "    'VERB': lexinfo['verb'],\n",
    "    'PRON': lexinfo['pronoun'],\n",
    "    'PROPN': lexinfo['properNoun'],\n",
    "    'ADP': lexinfo['adposition'],\n",
    "    'AUX': lexinfo['auxiliary'],\n",
    "    'CCONJ': lexinfo['coordinatingConjunction'],\n",
    "    'DET': lexinfo['determiner'],\n",
    "    'NUM': lexinfo['numeral'],\n",
    "    'PART': lexinfo['particle'],\n",
    "    'SCONJ': lexinfo['subordinatingConjunction'],\n",
    "    'PUNCT': lexinfo['punctuation'],\n",
    "    'SYM': lexinfo['symbol'],\n",
    "    'X': None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Lemma Bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a RDF file with a dataset holding our collection of Lemmas. Each lemma is defined as an instance of `ontolex:Form`, it is assigned to a `void:Dataset` for our collection and is provided with some basic description (a human-readable label, a written representation, a POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a **URN schema** for the lemmas: this will serve as URI for our objects. I adopt the [CITE2](https://brillpublishers.gitlab.io/documentation-cts/DTS_CITE_Explained.html) architecture, to make the URIs as independent as possible from implementation. In particular:\n",
    "- `urn:cite2`: specifies the protocol for the URN\n",
    "- `circselod`: setz the namespace to the CIRCSE service for linguistic linked data\n",
    "- `l2l.it`: sets the collection component to the Italian section of the `l2l` collection\n",
    "- `lemma_{nr}`: identifies the object using the DB id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdflib.term.URIRef('urn:cite2:circselod:l2l.it:lemma_1')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmurn = Namespace('urn:cite2:circselod:l2l.it:lemma_')\n",
    "lmurn['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize an empty `rdflib.Graph` to collect our triples. Then we add the first statements to our graph: we create a `void:Dataset` to collect our newly created lemma collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Nc54af6d5395e42fcb1a328ecd08e9737 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Graph()\n",
    "\n",
    "# Let's bind the namespaces\n",
    "g.bind('ontolex', ontolex)\n",
    "g.bind('dcterms', DCTERMS)\n",
    "g.bind('void', VOID)\n",
    "g.bind('lexinfo', lexinfo)\n",
    "\n",
    "dtset = URIRef('urn:cite2:circselod:l2l.it:lemma_bank')\n",
    "\n",
    "g.add((dtset, a, VOID.Dataset))\n",
    "g.add((dtset, RDFS.label, Literal('L2L Lemma Bank of Italian')))\n",
    "g.add((dtset, DCTERMS.title, Literal('L2L Lemma Bank of Italian')))\n",
    "g.add((dtset, VOID.vocabulary, URIRef('http://www.w3.org/ns/lemon/ontolex')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's populate the graph with the DB content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, st, pos in res:\n",
    "    l = lmurn[str(i)]\n",
    "    g.add((l, a, form))\n",
    "    g.add((l, written_rep, Literal(st)))\n",
    "    g.add((l, RDFS.label, Literal(st)))\n",
    "    g.add((l, has_pos, pos_map[pos]))\n",
    "    g.add((l, DCTERMS.isPartOf, dtset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412214"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voil√†! Now let's serialize the dataset as turtle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N0fab484eba3b417ab0a87dbb7550bd18 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.serialize('l2l_it_lemmma_bank.ttl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the candidate [frac](https://github.com/ontolex/frequency-attestation-corpus-information) extension of Ontolex to model the attestation. Our dataset will have include only the basic piece of infomation that a lexical entry canonically identified by a lemma from our previous graph (see above) has $n$ attestations in a given textual resource (defined as \"corpus\" by frac)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: `frac` requires to list also the total number of tokens of a corpus, so that the information is stored and it becomes possible to calculate the relative frequencies. At the moment, we did not collect this piece of data at the time of the DB generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how, according to the draft page, you register the frequency of all inflected form of a word in a text using `frac`:\n",
    "\n",
    "```turtle\n",
    "# word frequency, over all form variants \n",
    "epsd:kalag_strong_v a ontolex:LexicalEntry;\n",
    "    frac:frequency [\n",
    "        a frac:Frequency; \n",
    "        rdf:value \"2398\"^^xsd:int; \n",
    "        frac:observedIn <http://oracc.museum.upenn.edu/epsd2/pager>\n",
    "    ] .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URIs for the textual resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get the data from the `it_resource_descriptor` table of the DB. Do **rember** that, for reasons of space, the DB dump is limited to only 14 Italian textual resources from those available on the CLARIN's [VLO](https://vlo.clarin.eu/?0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'SELECT * from it_resource_descriptor;'\n",
    "cur.execute(q)\n",
    "res = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# this regexp should do to extract all the unique handle of the 14 CLARIN resources in the limited DB dump\n",
    "hdl_reg = re.compile(r'(https_58__47__47_\\S+)_64_format_61_cmdi;')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future convenience, we create a dictionary where the key is the numeric resource ID in the DB table and the value is the CLARIN handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_dict = {}\n",
    "\n",
    "for r in res:\n",
    "    try:\n",
    "        resource_dict[str(r[0])] = hdl_reg.findall(r[1])[0].replace('_58_', ':').replace('_47_', \"/\")\n",
    "    except IndexError:\n",
    "        print(f'handle not found: {r[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://hdl.handle.net/20.500.12124/6'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resource_dict['2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152239"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute('select id_resource, id_lemma, freq, it_lemmaBank.lemma from it_resource_lemma JOIN it_lemmaBank on id_lemma = it_lemmaBank.id;')\n",
    "res = cur.fetchall()\n",
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3049, 7, 'image')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Nbf14fb226b0f4b14bd108809bfdc97cd (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rdflib import XSD\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "# Let's bind the namespaces\n",
    "g.bind('ontolex', ontolex)\n",
    "g.bind('dcterms', DCTERMS)\n",
    "g.bind('void', VOID)\n",
    "g.bind('frac', frac)\n",
    "g.bind('xsd', XSD)\n",
    "\n",
    "dtset = URIRef('urn:cite2:circselod:l2l.it:corpus_frequencies')\n",
    "\n",
    "g.add((dtset, a, VOID.Dataset))\n",
    "g.add((dtset, DCTERMS.description, Literal('L2L frequency data of lemmas in some of CLARIN textual resources for Italian')))\n",
    "g.add((dtset, RDFS.label, Literal('L2L frequency data Italian')))\n",
    "g.add((dtset, DCTERMS.title, Literal('L2L frequency data Italian')))\n",
    "g.add((dtset, VOID.vocabulary, URIRef('http://www.w3.org/ns/lemon/ontolex')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```turtle\n",
    "# word frequency, over all form variants \n",
    "epsd:kalag_strong_v a ontolex:LexicalEntry;\n",
    "    frac:frequency [\n",
    "        a frac:Frequency; \n",
    "        rdf:value \"2398\"^^xsd:int; \n",
    "        frac:observedIn <http://oracc.museum.upenn.edu/epsd2/pager>\n",
    "    ] .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexurn = Namespace('urn:cite2:circselod:l2l.it:lex_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cid, lid, freq, lemmalab in res:\n",
    "    freqnode = BNode()\n",
    "    lex = lexurn[str(lid)]\n",
    "    g.add((lex, a, lexical_entry))\n",
    "    g.add((lex, canonical_form, lmurn[str(lid)]))\n",
    "    g.add((lex, RDFS.label, Literal(lemmalab)))\n",
    "    \n",
    "    # the frequency blank node\n",
    "    handle = URIRef(resource_dict[str(cid)])\n",
    "    g.add((freqnode, a, frac.Frequency))\n",
    "    g.add((freqnode, RDF.value, Literal(int(freq), datatype=XSD.integer)))\n",
    "    g.add((freqnode, frac.observedIn, handle))\n",
    "\n",
    "    g.add((lex, frac.frequency, freqnode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "856287"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's serialize the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Nbf14fb226b0f4b14bd108809bfdc97cd (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.serialize('l2l_it_frequencies.ttl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Lexical Resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, we link also one of the lexical resources that we identified in our [survey](). We chose the [OpenNER](http://hdl.handle.net/20.500.11752/ILC-73) sentiment lexicon for Italian. Once again, for convenience, the content of the dictionary has already been indexed and lemmatized by our [scraper]() and included in the DB dump (found in `data/CLARINDB.sql.tar.gz`). So, we'll get the data from the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25098"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute('select * from it_lex_res_elements')\n",
    "res = cur.fetchall()\n",
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 1,\n",
       " 'id_0\\tdi_cassetta\\tadj\\t0.333333333333\\tnegative',\n",
       " 'tsv_line',\n",
       " '{1}_{2}',\n",
       " '{4}')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results, however, are **not** linked to the lemma bank. We'll do that on the fly. Let's transform our lemma bank into a dictionary `lemma_pos:id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "cur.execute('select * from it_lemmaBank')\n",
    "lemma_dict = defaultdict(list)\n",
    "for i,lm,upos in cur.fetchall():\n",
    "    lemma_dict[f'{lm}_{upos.lower()}'].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemma_dict['sdruppo_noun'] # should be empty list\n",
    "lemma_dict['occupazione_noun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([d for d in lemma_dict.values() if len(d) > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, there are no duplicated couplets lemma,pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N31a33816575641d1b15507c58ed43cb1 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Graph()\n",
    "\n",
    "# Let's bind the namespaces\n",
    "g.bind('ontolex', ontolex)\n",
    "g.bind('dcterms', DCTERMS)\n",
    "g.bind('void', VOID)\n",
    "g.bind('lime', lime)\n",
    "g.bind('opener', 'http://hdl.handle.net/20.500.11752/ILC-73#')\n",
    "g.bind('marl', marl)\n",
    "\n",
    "dtset = URIRef('http://hdl.handle.net/20.500.11752/ILC-73')\n",
    "\n",
    "g.add((dtset, a, lime.Lexicon))\n",
    "g.add((dtset, RDFS.label, Literal('OpeNER Sentiment Lexicon Italian - LMF')))\n",
    "g.add((dtset, DCTERMS.title, Literal('OpeNER Sentiment Lexicon Italian - LMF')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what I want the final result to look like:\n",
    "\n",
    "```turtle\n",
    "opener:8 a ontolex:LexicalEntry ;\n",
    "    rdfs:label 'impotente_adj';\n",
    "    ontolex:canonicalForm <urn:cite2:circselod:l2l.it:lemma_39904> ;\n",
    "    ontolex:sense [ marl:hasPolarity marl:Negative ] .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model polarity, we use the same strategy used for the Latin Affectus polarity lexicon of Latin (as documented [here](https://zenodo.org/record/4067813))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "opener = Namespace('http://hdl.handle.net/20.500.11752/ILC-73#')\n",
    "\n",
    "for e in res:\n",
    "    entry_id = e[0]\n",
    "    entry_vals = e[2].split('\\t')\n",
    "    entry_key = f'{entry_vals[1]}_{entry_vals[2]}'\n",
    "    try:\n",
    "        entry_lm = lemma_dict[entry_key][0]\n",
    "    except IndexError:\n",
    "        continue\n",
    "    else:\n",
    "        entry_uri = opener[str(entry_id)]\n",
    "        g.add((entry_uri, a, ontolex.LexicalEntry))\n",
    "        g.add((entry_uri, RDFS.label, Literal(entry_key)))\n",
    "        g.add((entry_uri, ontolex.canonicalForm, lmurn[str(entry_lm)]))\n",
    "\n",
    "        # sense blank node\n",
    "        sense = BNode()\n",
    "        pol = entry_vals[-1].title()\n",
    "        g.add((sense, marl.hasPolarity, marl[pol]))\n",
    "\n",
    "        g.add((entry_uri, ontolex.sense, sense))\n",
    "        g.add((dtset, lime.entry, entry_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N31a33816575641d1b15507c58ed43cb1 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.serialize('l2l_open_ner.ttl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lila-kernel",
   "language": "python",
   "name": "lila-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
